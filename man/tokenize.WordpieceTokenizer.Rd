% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{tokenize.WordpieceTokenizer}
\alias{tokenize.WordpieceTokenizer}
\title{Tokenizer method for objects of WordpieceTokenizer class.}
\usage{
\method{tokenize}{WordpieceTokenizer}(wp_tokenizer, text)
}
\arguments{
\item{wp_tokenizer}{the WordpieceTokenizer object to apply.}

\item{text}{The text to tokenize. Should have already been passed through
BasicTokenizer.}
}
\value{
A list of wordpiece tokens.
}
\description{
Tokenizes a piece of text into its word pieces.
This uses a greedy longest-match-first algorithm to perform tokenization
using the given vocabulary.
For example:
 input = "unaffable"
 output = list("un", "##aff", "##able")
 ... although, ironically, the BERT vocabulary gives
 output = list("una", "##ffa", "##ble")
 for this example.
}
\examples{
\dontrun{
vocab <- load_vocab(vocab_file = "vocab.txt")
wp_tokenizer <- WordpieceTokenizer(vocab)
tokenize(wp_tokenizer, text = "a bunch of words")
}
}
